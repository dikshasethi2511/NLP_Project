# -*- coding: utf-8 -*-
"""BERT_CLASSIFIER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13s_yhJi2rvqc2WZ9ffpZjc20gXY3zU3d
"""

# using bert sequence classifier from hugging face 
# https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertForSequenceClassification
# !pip install transformers

# from google.colab import drive
# drive.mount('/content/drive')

import torch
from transformers import BertTokenizer, BertForSequenceClassification 
import pandas as pd
import os
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
from sklearn.metrics import f1_score

# dataset = np.array(pd.read_csv('/content/drive/MyDrive/NLP_Project_sem5/processed_data.csv'))
# dataset = pd.read_csv('/content/drive/MyDrive/NLP_Project_sem5/processed_data.csv')

train_dataset = pd.read_csv('/ssd-scratch/vibhu20150/temp/Datasets-Processed/H3_Multiclass_Hate_Speech_Detection_train_preprocessed.csv')
test_dataset = pd.read_csv('/ssd-scratch/vibhu20150/temp/Datasets-Processed/H3_Multiclass_Hate_Speech_Detection_test_preprocessed.csv')

print(train_dataset)
print(test_dataset)

print(train_dataset.shape)
# print(dataset[0,2])
print(train_dataset)
print(test_dataset.shape)
# print(dataset[0,2])
print(test_dataset)

# num_labels = 3
# tokenizer = BertTokenizer.from_pretrained("textattack/bert-base-uncased-yelp-polarity")

# model = BertForSequenceClassification.from_pretrained("textattack/bert-base-uncased-yelp-polarity", problem_type="multi_label_classification", )

# print(model.config.id2label)



from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, dataset, tokenizer, class_type="all"):
        if class_type == "all":
            self.labels = [label for label in dataset['label']]
            self.tweets = [tokenizer(tweet, max_length=280, padding='max_length', truncation=True, return_tensors="pt") for tweet in dataset['tweet']]
        elif class_type == "hate":
            self.labels = [label for label in dataset['label'] if label == 0]
            self.tweets = [tokenizer(row[1], max_length=280, padding='max_length', truncation=True, return_tensors="pt") for row in np.array(dataset) if row[0] == 0] 


    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        tweet = self.tweets[idx]
        label = self.labels[idx]

        return tweet, label

    def getLabels(self):
        return self.labels

# sequence classification/regression head on top a linear layer on top of the pooled output of BERT
from transformers import BertModel


class BertSeqPoolLinClassifier(nn.Module):
    def __init__(self):
        super(BertSeqPoolLinClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.linear_layer = nn.LazyLinear(3)

        # self.pooling_layer = nn.AvgPool1d(384, stride=192)
        self.pooling_layer = nn.AvgPool1d(8, stride=8)
        self.conv_layer = nn.Conv1d(in_channels=280, out_channels=100, kernel_size=10, stride=8)
        self.conv_layer_2d = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(5,280))
        self.avg_pool_2d = nn.AvgPool2d((5,5), stride=(4,4))
        self.lin_layer_all = nn.Linear(768, 384)
        self.lin_layer_reduction = nn.LazyLinear(384)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.tan_h = nn.Tanh()
        self.dropout = nn.Dropout(0.1)

        # self.soft_max = nn.Softmax(dim=3) 

    def forward(self, input_ids, bert_mask):
        seq_last_hidden_states, pooled_output, all_hidden_layers = self.bert(input_ids=input_ids, attention_mask=bert_mask, return_dict=False, output_hidden_states=True)
        # print(f"output hidden states shape:   {all_hidden_layers[0].shape}, {all_hidden_layers[0].shape}, {len(all_hidden_layers)}")
        # print("SEQ SHAPE: ", seq_last_hidden_states.shape) 
        # print("Pool Shape: ", pooled_output.shape)
        # pooled_hidden_states = self.pooling_layer(seq_last_hidden_states)
        seq_last_hidden_states = self.dropout(seq_last_hidden_states) 
        # conv_output = self.conv_layer(seq_last_hidden_states)
        
        # print("DONE")
        combined_hidden_layers = torch.cat([hidden_layer.unsqueeze(0) for hidden_layer in all_hidden_layers], dim=0)
        combined_hidden_layers = combined_hidden_layers.permute((1, 0, 2, 3))
        lin_combined_hidden_layers = self.lin_layer_all(combined_hidden_layers) 
        lin_combined_hidden_layers = self.dropout(lin_combined_hidden_layers)
        conv2d_output = self.conv_layer_2d(lin_combined_hidden_layers)
        
        # print(f"combine layers shape: {combined_hidden_layers.shape}")
        # print(f"lin combine layers shape: {lin_combined_hidden_layers.shape}")
        # print(f"conv combine layers shape: {conv2d_output.shape}")
        conv2d_output = self.avg_pool_2d(conv2d_output)
        conv2d_output = conv2d_output.reshape(conv2d_output.shape[0], conv2d_output.shape[1], -1)

        # conv 2d 

        # print("Conv output: ", conv_output.shape)
        conv2d_output = conv2d_output.reshape(pooled_output.shape[0], -1)
        linear_output = self.relu(conv2d_output)
        linear_output = self.relu(self.linear_layer(linear_output))

        return linear_output

from tqdm import tqdm
from torch.utils.data import DataLoader

def train(model, train_data, test_data, learning_rate_bert, learning_rate_lin, epochs, device):
    model.train()
    loss_function = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW([
        {'params': model.bert.parameters(), 'lr': learning_rate_bert},
        {'params': model.linear_layer.parameters()},
        # {'params': model.conv_layer.parameters()},
        {'params': model.conv_layer_2d.parameters()},
        {'params': model.lin_layer_all.parameters()},
        {'params': model.lin_layer_reduction.parameters()}
        ], lr=learning_rate_lin)

    if(device == "cuda"):
        model.cuda()
        loss_function.cuda()

    
    batch_size = 8
    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    test_dataloader = DataLoader(test_data, batch_size=batch_size)
    
    best_score = 0.0
    
    for epoch in tqdm(range(epochs)):

        correct_preds = 0
        train_preds = []
        test_preds = []

        for batch, (tweets, labels) in enumerate(train_dataloader):
            
            labels = labels.to(device)
            # print()
            # print("="*40)
            # print()
            # print(tweets["attention_mask"].shape)
            # print()
            output = model(tweets["input_ids"].squeeze(1).to(device), tweets["attention_mask"].to(device))
            # print(f"label shape: {labels.shape}") # should be batch X 1
            loss = loss_function(output, labels) 
            # print("output shape: ", output.shape)
            
            # preds = model() 

            # update
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()


            # accuracy 
            preds = output.argmax(dim=1)
            # print("preds shape: ", preds.shape)
            # print("labels shape: ", len(labels))

            for i in range(len(labels)):
                # print(i)
                train_preds.append(preds[i].cpu())
                if preds[i] == labels[i]:
                    correct_preds += 1
            
        
        


        # testing code 

        test_correct_preds = 0

        with torch.no_grad():
            for batch, (tweets, labels) in enumerate(test_dataloader):
                labels = labels.to(device)
                # print()
                # print("="*40)
                # print()
                # print(tweets["attention_mask"].shape)
                # print()
                output = model(tweets["input_ids"].squeeze(1).to(device), tweets["attention_mask"].to(device))
                # print(f"label shape: {labels.shape}") # should be batch X 1
                # loss = loss_function(output, labels)
                # print("output shape: ", output.shape)
                
                # preds = model() 
                # accuracy 
                preds = output.argmax(dim=1)
                # print("preds shape: ", preds.shape)
                # print("labels shape: ", labels.shape)

                for i in range(len(labels)):
                    # print(i)
                    test_preds.append(preds[i].cpu())
                    if preds[i] == labels[i]:
                        test_correct_preds += 1
        

          
        print(f"\nEpoch: {epoch + 1}, Train Acc: {correct_preds/len(train_data)}, Train Length: {len(train_data)}, Test Acc: {test_correct_preds/len(test_data)}, Test Length: {len(test_data)}")
        print(f"F1 Score Train: {f1_score(train_data.getLabels(), train_preds, average=None)}")
        print(f"Macro F1 Score Train: {f1_score(train_data.getLabels(), train_preds, average='macro')}")
        print(f"F1 Score Test: {f1_score(test_data.getLabels(), test_preds, average=None)}")
        print(f"Macro F1 Score Test: {f1_score(test_data.getLabels(), test_preds, average='macro')}")
        curr_score = f1_score(test_data.getLabels(), test_preds, average='macro') 

        if curr_score > best_score:
            best_score = curr_score
            torch.save(model, "./conv-bert-lin-relu-best.pth")
        # save best model


# !nvidia-smi

device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
torch.cuda.current_device()
torch.cuda.get_device_name(torch.cuda.current_device())
# print()
# for row in np.array(train_dataset)[:1]:
#     print(row.shape)
#     print(row[0], row[1])
#     exit()
    # print(row['tweet'])

model = BertSeqPoolLinClassifier()
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# model = torch.load("/ssd-scratch/vibhu20150/temp/pooled-bert-lin-3-best.pth")

print(len([para for para in model.parameters()]))
print(len([para for para in model.bert.parameters()]))
print(len([para for para in model.linear_layer.parameters()]))

labels_map = {0: "hate?",
              1: "offensive?",
              2: "none?",
              }

# target = torch.empty(3, dtype=torch.long).random_(5)
# print(target)
# print(target.shape)
# print(torch.device("cuda"))

# print(dataset)
# print(tokenizer)
# train_data = CustomDataset(train_dataset, tokenizer)



train_data = CustomDataset(train_dataset[:17842], tokenizer) 
test_data = CustomDataset(train_dataset[17842:], tokenizer)

print("TRAIN LENGTH: ", len(train_data))

train(model, train_data, test_data, 0.00001, 0.001, 30, device)


# torch.save(model.state_dict(), "./trained-model-states.pth")
# torch.save(model, "./pooled-bert-3.pth")