{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAQZtryJ-I9T",
        "outputId": "15fd5343-8067-4cce-eb77-450d29cf4eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "# using bert sequence classifier from hugging face \n",
        "# https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertForSequenceClassification\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWfHRPtdgaYz",
        "outputId": "ae95c022-12fb-4a0e-a756-9a7b3cbf02e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification \n",
        "import pandas as pd\n",
        "import os\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "1WRD4Y3d-ShV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset = np.array(pd.read_csv('/content/drive/MyDrive/NLP_Project_sem5/processed_data.csv'))\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/NLP_Project_sem5/processed_data.csv')"
      ],
      "metadata": {
        "id": "PwuIA4LTAdKk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.shape)\n",
        "# print(dataset[0,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm5y-QqnA-rs",
        "outputId": "b1b4c8e3-ce87-4b2f-8df5-156960519352"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19826, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# num_labels = 3\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\", problem_type=\"multi_label_classification\", )"
      ],
      "metadata": {
        "id": "Ugy8ZJcx_07T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model.config.id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1rpjot_DetY",
        "outputId": "98fa74b0-4395-4d7c-b5aa-e18182b1bbd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = tokenizer(dataset['tweet'][0], return_tensors=\"pt\")\n",
        "# print(inputs.items())\n",
        "# print(inputs['input_ids'].squeeze(1).shape)\n",
        "# print(tokenizer.decode(inputs[\"input_ids\"][0]))\n",
        "# logits = model(**inputs).logits\n",
        "# print(logits)\n",
        "# predicted_class_id = logits.argmax().item()\n",
        "# print(predicted_class_id)\n",
        "# model.config.id2label[predicted_class_id]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "EyNVaPaJCHKL",
        "outputId": "f8d46915-ec69-42be-c11d-b1141ba90ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('input_ids', tensor([[  101,  1026,  5310, 18442,  1028,  1048,  2213,  7011,  2080,  2023,\n",
            "          4937,  2318,  6012,  1996,  4485,  2041,  1997,  2033,  2026,  9152,\n",
            "         23033,  2633,  2288,  2070, 22418,  2026,  9152, 23033,   102]])), ('token_type_ids', tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1]]))])\n",
            "torch.Size([1, 29])\n",
            "[CLS] < username > lmfao this cat started beating the shit out of me my nigga finally got some pussy my nigga [SEP]\n",
            "tensor([[-2.5881,  2.9254]], grad_fn=<AddmmBackward0>)\n",
            "1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LABEL_1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.labels = [label for label in dataset['label']]\n",
        "        self.tweets = [tokenizer(tweet, max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\") for tweet in dataset['tweet']]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.tweets[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return tweet, label\n",
        "\n",
        "    def getLabels(self):\n",
        "        return self.labels\n",
        "    \n"
      ],
      "metadata": {
        "id": "hQD7-7bIjHK6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sequence classification/regression head on top a linear layer on top of the pooled output of BERT\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "class BertSeqClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertSeqClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.linear_layer = nn.Linear(768, 3)\n",
        "        self.soft_max = nn.Softmax(dim=3) \n",
        "\n",
        "    def forward(self, input_ids, bert_mask):\n",
        "        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=bert_mask, return_dict=False)\n",
        "        linear_output = self.linear_layer(pooled_output)\n",
        "        # probs = self.soft_max(linear_output) # already incorporated in crossentropyLoss\n",
        "        # return probs\n",
        "        return linear_output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0UUR8QOpKmJ5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(model, train_data, test_data, learning_rate, epochs, device):\n",
        "    model.train()\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    if(device == \"cuda\"):\n",
        "        model.cuda()\n",
        "        loss_function.cuda()\n",
        "\n",
        "    \n",
        "    batch_size = 8\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    \n",
        "    \n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        correct_preds = 0\n",
        "        train_preds = []\n",
        "        test_preds = []\n",
        "\n",
        "        for batch, (tweets, labels) in enumerate(train_dataloader):\n",
        "            \n",
        "            labels = labels.to(device)\n",
        "            # print()\n",
        "            # print(\"=\"*40)\n",
        "            # print()\n",
        "            # print(tweets[\"attention_mask\"].shape)\n",
        "            # print()\n",
        "            output = model(tweets[\"input_ids\"].squeeze(1).to(device), tweets[\"attention_mask\"].to(device))\n",
        "            # print(f\"label shape: {labels.shape}\") # should be batch X 1\n",
        "            loss = loss_function(output, labels)\n",
        "            # print(\"output shape: \", output.shape)\n",
        "            \n",
        "            # preds = model() \n",
        "\n",
        "            # update\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # accuracy \n",
        "            preds = output.argmax(dim=1)\n",
        "            # print(\"preds shape: \", preds.shape)\n",
        "            # print(\"labels shape: \", len(labels))\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                # print(i)\n",
        "                train_preds.append(preds[i].cpu())\n",
        "                if preds[i] == labels[i]:\n",
        "                    correct_preds += 1\n",
        "            \n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        # testing code \n",
        "\n",
        "        test_correct_preds = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch, (tweets, labels) in enumerate(test_dataloader):\n",
        "                labels = labels.to(device)\n",
        "                # print()\n",
        "                # print(\"=\"*40)\n",
        "                # print()\n",
        "                # print(tweets[\"attention_mask\"].shape)\n",
        "                # print()\n",
        "                output = model(tweets[\"input_ids\"].squeeze(1).to(device), tweets[\"attention_mask\"].to(device))\n",
        "                # print(f\"label shape: {labels.shape}\") # should be batch X 1\n",
        "                loss = loss_function(output, labels)\n",
        "                # print(\"output shape: \", output.shape)\n",
        "                \n",
        "                # preds = model() \n",
        "                # accuracy \n",
        "                preds = output.argmax(dim=1)\n",
        "                # print(\"preds shape: \", preds.shape)\n",
        "                # print(\"labels shape: \", labels.shape)\n",
        "\n",
        "                for i in range(len(labels)):\n",
        "                    # print(i)\n",
        "                    test_preds.append(preds[i].cpu())\n",
        "                    if preds[i] == labels[i]:\n",
        "                        test_correct_preds += 1\n",
        "        \n",
        "\n",
        "          \n",
        "        print(f\"\\nEpoch: {epoch + 1}, Train Acc: {correct_preds/len(train_data)}, Train Length: {len(train_data)}, Test Acc: {test_correct_preds/len(test_data)}, Test Length: {len(test_data)}\")\n",
        "        print(f\"F1 Score Train: {f1_score(train_data.getLabels(), train_preds, average=None)}\")\n",
        "        print(f\"Macro F1 Score Train: {f1_score(train_data.getLabels(), train_preds, average='macro')}\")\n",
        "        print(f\"F1 Score Test: {f1_score(test_data.getLabels(), test_preds, average=None)}\")\n",
        "        print(f\"Macro F1 Score Test: {f1_score(test_data.getLabels(), test_preds, average='macro')}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SiHwwRPaSNPk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCmuGqx0qIUM",
        "outputId": "ab631c87-ccb7-4189-f8c9-60a019653695"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 22 10:31:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "torch.cuda.current_device()\n",
        "torch.cuda.get_device_name(torch.cuda.current_device())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "eE-O34MjVGAQ",
        "outputId": "55386913-51a8-43b0-fc75-1bd5710ecd0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQ_RFTRhhEg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {0: \"hate?\",\n",
        "              1: \"offensive?\",\n",
        "              2: \"none?\",\n",
        "              }\n",
        "\n",
        "# target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "# print(target)\n",
        "# print(target.shape)\n",
        "# print(torch.device(\"cuda\"))\n",
        "\n",
        "\n",
        "model = BertSeqClassifier()\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# print(dataset)\n",
        "# print(tokenizer)\n",
        "train_data = CustomDataset(dataset[:10000], tokenizer)\n",
        "test_data = CustomDataset(dataset[10000:], tokenizer)\n",
        "\n",
        "\n",
        "train(model, train_data, test_data, 0.00001, 3, device)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1Fb8uzUicrm",
        "outputId": "e86f6bb1-5324-4c03-e03e-c3770e91c680"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            " 33%|███▎      | 1/3 [02:03<04:06, 123.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1, Train Acc: 0.787, Train Length: 1000, Test Acc: 0.819, Test Length: 1000\n",
            "F1 Score Train: [0.         0.88080582 0.        ]\n",
            "Macro F1 Score Train: 0.293601939936579\n",
            "F1 Score Test: [0.         0.90083102 0.08571429]\n",
            "Macro F1 Score Test: 0.3288484368816779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [04:06<02:03, 123.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2, Train Acc: 0.87, Train Length: 1000, Test Acc: 0.892, Test Length: 1000\n",
            "F1 Score Train: [0.03125    0.79752322 0.11214953]\n",
            "Macro F1 Score Train: 0.31364091750817397\n",
            "F1 Score Test: [0.06896552 0.94427434 0.77022654]\n",
            "Macro F1 Score Test: 0.5944887987201987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [06:09<00:00, 123.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3, Train Acc: 0.913, Train Length: 1000, Test Acc: 0.886, Test Length: 1000\n",
            "F1 Score Train: [0.02597403 0.79043424 0.16766467]\n",
            "Macro F1 Score Train: 0.32802431066695487\n",
            "F1 Score Test: [0.18461538 0.94117647 0.75964392]\n",
            "Macro F1 Score Test: 0.6284785907058555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}